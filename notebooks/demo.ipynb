{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0556f4-3bb7-4243-bd3a-d98228b7dc38",
   "metadata": {},
   "source": [
    " # Luonnollisen kielen käsittelyn (NLP) demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777a40e-ba18-4b92-bc79-31b4826e9394",
   "metadata": {},
   "source": [
    "## Lähteitä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059463da-581d-40a1-83b1-3cb602aa8bb1",
   "metadata": {},
   "source": [
    "Koodia, esimerkkejä:\n",
    "\n",
    "- Demo mukailee esimerkkitutorialia: https://www.tensorflow.org/hub/tutorials/tf2_text_classification\n",
    "- Osia myös tutorialista: https://medium.com/intro-to-artificial-intelligence/entity-extraction-using-deep-learning-8014acac6bb8\n",
    "- ...ja täältä: https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85-\n",
    "- ...ja täältä: https://github.com/kcmankar/pytorch-sentiment-analysis-using-XLNet/blob/master/xlnet_sentiment_analysis.ipynb\n",
    "- .. ja täältä: https://news.machinelearning.sg/posts/sentiment_analysis_on_movie_reviews_with_xlnet/\n",
    "\n",
    "Dataa: \n",
    "\n",
    "- Sentimenttidata: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb\n",
    "- Sentimenttidata, alkuperäinen lähde? : https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "- IMDb-data: https://www.imdb.com/interfaces/\n",
    "\n",
    "Menetelmiä:\n",
    "\n",
    "- XLNet: https://github.com/zihangdai/xlnet\n",
    "- XLNet-paperi: https://arxiv.org/pdf/1906.08237.pdf\n",
    "- GLUE-sivusto: https://gluebenchmark.com/\n",
    "- GLUE-paperi: https://openreview.net/pdf?id=rJ4km2R5t7\n",
    "\n",
    "Muita lähteitä:\n",
    "\n",
    "- NLP-kehitystä seuraileva GitHub-repo: https://github.com/sebastianruder/NLP-progress\n",
    "- Toinen NLP-GitHub -repo: https://github.com/keon/awesome-nlp\n",
    "- Turku NLP: https://turkunlp.org/\n",
    "- Kieliriippumattomia sana-assosiaatioita https://universaldependencies.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f9ea3-1627-466f-ac74-db301473f042",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3fe870-b2c5-47a5-90ee-1657ca0c07ee",
   "metadata": {},
   "source": [
    "Määritellään tarvittavat Python-kieliset paketit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24580b-d93e-4c07-92a1-8a31be49f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# NNLP:tä varten:\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Kytketään pois GPU:n puuttumisesta kertovat virheviestit.\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "# XLNetiä varten:\n",
    "\n",
    "#  import transformers\n",
    "# from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
    "# import torch\n",
    "\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "# from matplotlib import rc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, accuracy_score # accuracy vai accuracy_score?\n",
    "# from sklearn.utils import shuffle\n",
    "# from collections import defaultdict\n",
    "# from textwrap import wrap\n",
    "#from pylab import rcParams\n",
    "\n",
    "# from torch import nn, optim\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a627a-168f-4ecc-bfb9-7298527b86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n",
    "                                  batch_size=-1, as_supervised=True)\n",
    "train_examples, train_labels = tfds.as_numpy(train_data)\n",
    "test_examples, test_labels = tfds.as_numpy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d276df-9336-436f-84e9-c89b5427a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7c8a3-2d1d-4384-b271-cdc33d1dd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01744a2d-78ed-44ea-8509-a3b5d32ff0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034cc25-dc9e-4d79-920b-2dea2c25a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06d5b0-5f42-4e7f-9c57-83cac0c16f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8aa29b-e292-4451-b1e3-a66a2e9f5eaa",
   "metadata": {},
   "source": [
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "# hub_layer(train_examples[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c9032-013c-4466-b69a-ac9a7dfdb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a64f94-2d44-467c-ac6c-d9bd9be958bd",
   "metadata": {},
   "source": [
    "![title](neural_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957deeb-57f9-4ebb-95bf-688383a01228",
   "metadata": {},
   "source": [
    "Lähde https://www.cs.mcgill.ca/~jcheung/teaching/fall-2016/comp599/lectures/lecture23.pdf \"Goldberg et al. 2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a502ee2-dfc0-44d4-af47-14c7d270592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e00b48-2dd6-4334-8297-88c912f99bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = 10000\n",
    "\n",
    "x_val = train_examples[:train_cutoff]\n",
    "partial_x_train = train_examples[train_cutoff:]\n",
    "\n",
    "y_val = train_labels[:train_cutoff]\n",
    "partial_y_train = train_labels[train_cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561133c-44bc-4753-bdc7-884b682f94b5",
   "metadata": {},
   "source": [
    "Mallin opetusta (kesto olisi n. 12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa1f2b-42dc-459d-8889-bc2f51250598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../work/nlp_model'\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    model = keras.models.load_model(model_dir)\n",
    "\n",
    "    with open(os.path.join(model_dir, 'hist.pkl'), 'rb') as handle:\n",
    "        hist = pickle.load(handle)\n",
    "else:\n",
    "    history = model.fit(partial_x_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=40,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1)\n",
    "    model.save(model_dir)\n",
    "                                \n",
    "    with open(os.path.join(model_dir, 'hist.pkl'), 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    hist = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cacf3fe-421f-49d8-b4c8-54793f33f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb226ec-760c-4ea2-9ee5-b2b5c30a00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = hist\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c30ad-9f05-45c6-a797-338bd6fad801",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c5c4e-5c29-4de2-9f9c-4aad7cba3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28333c-f7f3-4b31-8c97-818c2dacd2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../data/IMDb_kaggle/IMDB Dataset.csv'\n",
    "df = pd.read_csv(path_to_data)\n",
    "\n",
    "# Shuffle and Clip data\n",
    "df = shuffle(df)\n",
    "df = df[:24000]\n",
    "\n",
    "# Function to clean text. Remove tagged entities, hyperlinks, emojis\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text\n",
    " \n",
    "df['review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Function to convert labels to number.\n",
    "def sentiment2label(sentiment):\n",
    "    if sentiment == \"positive\":\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(sentiment2label)\n",
    "\n",
    "# List of class names.\n",
    "class_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913911a-5b03-49b8-8341-adfa3d25f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e2e3f-982e-4dea-a2f1-f190762c9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2288666-5de5-4aee-8548-74344a8921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = pad_sequences(encoding['input_ids'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "        input_ids = input_ids.astype(dtype = 'int64')\n",
    "        input_ids = torch.tensor(input_ids) \n",
    "\n",
    "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
    "        attention_mask = torch.tensor(attention_mask)       \n",
    "\n",
    "        return {\n",
    "        'review_text': review,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "        'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787e8d4-cb18-4024-b53c-4849f2b2f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.5, random_state=101)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0c8a9-884d-4edd-8e4a-72a4b5b8a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = ImdbDataset(\n",
    "    reviews=df.review.to_numpy(),\n",
    "    targets=df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09dea4c-780a-4865-bae0-77104eaed331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff04f0b-94e4-4d73-a092-0ad1bc5e47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "MAX_LEN = 512\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646e011-07ec-4755-8a74-616bba7f431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetForSequenceClassification\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9ce78-5990-40ae-9100-bcdc9b257390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688d1d3-ccea-4ecf-96e5-e14932075835",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4d67c-33f4-47e2-8c33-e9fcb9e2cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75c9c8-4398-47bd-a901-4098d895c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(val_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7c15c-87be-4ff6-a62a-b7084f6809e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb0e81-f3c9-4921-a2c3-fad42eda58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe5954-5713-4026-b412-a3cbfdbfe346",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../data/sentiment/amazon_cells_labelled.txt'\n",
    "fd = pd.read_csv(path_to_data, sep='\\t')\n",
    "fd.columns = ['sentence','value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9941fc2-ffdb-49ed-995d-669c822092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences  = []\n",
    "for sentence in fd['sentence']:\n",
    "  sentence = sentence+\"[SEP] [CLS]\"\n",
    "  sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db483bc6-58f3-47b6-83a3-ebd191ea0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee7ad0-bfc5-44b0-b139-f2e8e0d1c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import XLNetTokenizer,XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca5510-91dc-4e17-9cad-f9fd2850beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9dca0-c862-487b-a570-65999467863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True)\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab7e09-2de7-449b-be6e-bd546c107105",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7c14c-bda1-4d2c-9b4b-9fc6b1a940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecb064-b61b-43cf-9b92-ce872872c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(ids[0])\n",
    "labels = fd['value'].values\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18da0a1-f985-457e-8613-6095f4cf1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max1 = len(ids[0])\n",
    "for i in ids:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "print(max1)\n",
    "MAX_LEN = max1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50335715-1b04-4f46-a96d-918cb69f83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids2 = pad_sequences(ids,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9b1e1-497f-43c8-a71e-dc6139bde964",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(input_ids2,labels,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce64da-91db-4ef7-958a-d9d084f63371",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_ids2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419e9ef-74d5-4a66-a0e2-b370a0ad9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = torch.tensor(xtrain)\n",
    "Ytrain = torch.tensor(ytrain)\n",
    "Xtest = torch.tensor(xtest)\n",
    "Ytest = torch.tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf5d69-7d01-4e7f-9364-59605a8329e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675aabd-c5b0-4a21-a784-1fe94605c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(Xtrain,Ytrain)\n",
    "test_data = TensorDataset(Xtest,Ytest)\n",
    "loader = DataLoader(train_data,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbf21d-667a-4539-a22f-b4544ecb3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef4fe5-9a25-4db4-9852-cdaa4cac9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066dfce-e992-4274-a65a-f537b5bd7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93a3e8-7e8d-42e6-b8e1-61e313544d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059abcdf-e9ef-49bb-8846-c3ba464880e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def flat_accuracy(preds,labels):  # A function to predict Accuracy\n",
    "  correct=0\n",
    "  for i in range(0,len(labels)):\n",
    "    if(preds[i]==labels[i]):\n",
    "      correct+=1\n",
    "  return (correct/len(labels))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25024f-79d4-424e-8cde-2eb44f16d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_train = 0\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  loss1 = []\n",
    "  steps = 0\n",
    "  train_loss = []\n",
    "  l = []\n",
    "  for inputs,labels1 in loader :\n",
    "    inputs.to(device)\n",
    "    labels1.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs.to(device))\n",
    "    loss = criterion(outputs[0],labels1.to(device)).to(device)\n",
    "    logits = outputs[1]\n",
    "    #ll=outp(loss)\n",
    "    [train_loss.append(p.item()) for p in torch.argmax(outputs[0],axis=1).flatten() ]#our predicted \n",
    "    [l.append(z.item()) for z in labels1]# real labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss1.append(loss.item())\n",
    "    no_train += inputs.size(0)\n",
    "    steps += 1\n",
    "  print(\"Current Loss is : {} Step is : {} number of Example : {} Accuracy : {}\".format(loss.item(),epoch,no_train,flat_accuracy(train_loss,l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf427-8c75-477a-aad7-332c977a4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()#Testing our Model\n",
    "acc = []\n",
    "lab = []\n",
    "t = 0\n",
    "for inp,lab1 in test_loader:\n",
    "  inp.to(device)\n",
    "  lab1.to(device)\n",
    "  t+=lab1.size(0)\n",
    "  outp1 = model(inp.to(device))\n",
    "  [acc.append(p1.item()) for p1 in torch.argmax(outp1[0],axis=1).flatten() ]\n",
    "  [lab.append(z1.item()) for z1 in lab1]\n",
    "print(\"Total Examples : {} Accuracy {}\".format(t,flat_accuracy(acc,lab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e871a2-c109-4773-b6d3-f79618d54069",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q simpletransformers\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552be0f-dbad-4093-9b50-8e26629b44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_train = load_dataset('imdb',split='train')\n",
    "dataset_train.rename_column_('label', 'labels')\n",
    "train_df=pd.DataFrame(dataset_train)\n",
    "\n",
    "dataset_test = load_dataset('imdb',split='test')\n",
    "dataset_test.rename_column_('label', 'labels')\n",
    "test_df=pd.DataFrame(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60cfe8-b81b-49ea-a83b-3d53be28eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca4431-1b90-41bc-8c33-b6d1be81a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[train_df.labels.value_counts()[0], test_df.labels.value_counts()[0]], \n",
    "        [train_df.labels.value_counts()[1], test_df.labels.value_counts()[1]]]\n",
    "# Prints out the dataset sizes of train test and validate as per the table.\n",
    "pd.DataFrame(data, columns=[\"Train\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bd01c-fc85-4747-b52a-5c1c1f77f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'sliding_window': True,\n",
    "    'max_seq_length': 64,\n",
    "    'num_train_epochs': 1,\n",
    "    'learning_rate': 0.00001,\n",
    "    'weight_decay': 0.01,\n",
    "    'train_batch_size': 128,\n",
    "    'fp16': True,\n",
    "    'output_dir': '../work/xlnet_outputs/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282574d-44b4-42d3-9670-410c399e9d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e939dbd-6928-477a-8b7e-e8b1fb4ec897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sklearn\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "transformers_logger = logging.getLogger('transformers')\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# We use the XLNet base cased pre-trained model.\n",
    "model = ClassificationModel('xlnet', 'xlnet-base-cased', num_labels=2, args=train_args, use_cuda=False) \n",
    "\n",
    "# Train the model, there is no development or validation set for this dataset \n",
    "# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping\n",
    "model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model in terms of accuracy score\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9fb43-c80b-493f-9f09-9803f3079311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
